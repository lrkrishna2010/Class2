{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57440182",
   "metadata": {},
   "source": [
    "# Week 1 — Interactive Quant Risk & Optimization (v4.2 Educational Edition, Live Data)\n",
    "\n",
    "Welcome! This **hands-on notebook** explains both the *math* and the *intuition* of portfolio risk and optimization.\n",
    "\n",
    "### What you’ll learn\n",
    "1. **Measure risk**: volatility, Sharpe, VaR, CVaR, drawdowns\n",
    "2. **Optimize portfolios**: Equal-Weight vs. Mean–Variance (risk aversion $\\gamma$)\n",
    "3. **Validate & interpret**: cumulative & drawdown curves, rolling Sharpe\n",
    "4. **Explore interactively**: sliders for VaR confidence (α) and risk aversion (γ)\n",
    "\n",
    "### Flow of the notebook\n",
    "**Data → Returns → Risk → Optimization → Backtest → Interpretation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740b315",
   "metadata": {},
   "source": [
    "## 0) Setup & Helpers\n",
    "This section prepares the plotting style, optional libraries, and utility functions.\n",
    "\n",
    "**Why these choices?**\n",
    "- **Matplotlib** is reliable for static figures; **Plotly** adds zoom/hover when available.\n",
    "- **ipywidgets** enables interactive sliders for teaching intuition.\n",
    "- We include **quick-export** so you can save figures into `reports/` for slides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (9,4)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Optional libs (Plotly & ipywidgets): detected at runtime\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    _HAVE_PLOTLY = True\n",
    "except Exception:\n",
    "    _HAVE_PLOTLY = False\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    _HAVE_WIDGETS = True\n",
    "except Exception:\n",
    "    _HAVE_WIDGETS = False\n",
    "\n",
    "# === Figure export helper ===\n",
    "def export_fig(fig=None, name='chart', fmt='png', out_dir='reports'):\n",
    "    \"\"\"Save the current or provided matplotlib figure to reports/ as PNG/PDF.\"\"\"\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    path = Path(out_dir) / f\"{name}.{fmt}\"\n",
    "    fig.savefig(path, bbox_inches='tight', dpi=300)\n",
    "    print(f\" Exported chart to {path}\")\n",
    "\n",
    "# === Summary stats under charts ===\n",
    "def show_summary_stats(returns_series: pd.Series):\n",
    "    r = returns_series.dropna()\n",
    "    ann_ret = r.mean()*252\n",
    "    ann_vol = r.std()*np.sqrt(252)\n",
    "    sharpe  = ann_ret/(ann_vol+1e-12)\n",
    "    cum = (1+r).cumprod()\n",
    "    mdd = (cum/cum.cummax()-1.0).min()\n",
    "    print(f\" Annualized Return: {ann_ret:.2%}\")\n",
    "    print(f\" Volatility: {ann_vol:.2%}\")\n",
    "    print(f\" Sharpe Ratio: {sharpe:.2f}\")\n",
    "    print(f\" Max Drawdown: {mdd:.2%}\")\n",
    "\n",
    "# === Rolling Sharpe ===\n",
    "def rolling_sharpe(r: pd.Series, window=252):\n",
    "    mu = r.rolling(window).mean()\n",
    "    sd = r.rolling(window).std()\n",
    "    return (np.sqrt(252)*mu)/(sd+1e-12)\n",
    "\n",
    "# === Simple backtest: monthly rebalancing, transaction costs (bps) ===\n",
    "def rebalance_backtest(returns: pd.DataFrame, weight_fn, rebalance_freq='M', tx_bps: float = 5.0):\n",
    "    idx = returns.resample(rebalance_freq).last().index\n",
    "    weights_rec, port_rets = [], []\n",
    "    prev_w = None\n",
    "    for i, end in enumerate(idx):\n",
    "        window = returns.loc[:end]\n",
    "        if window.empty: continue\n",
    "        w = weight_fn(window)\n",
    "        w = w/(w.sum()+1e-12)\n",
    "        weights_rec.append((end, w))\n",
    "        nxt = idx[i+1] if i+1 < len(idx) else returns.index[-1]\n",
    "        seg = returns.loc[(returns.index> end) & (returns.index<=nxt)]\n",
    "        if seg.empty: continue\n",
    "        pr = seg @ w\n",
    "        if prev_w is not None and tx_bps>0:\n",
    "            turnover = np.abs(w - prev_w).sum()\n",
    "            if len(pr)>0:\n",
    "                pr.iloc[0] -= (tx_bps/10000.0)*turnover\n",
    "        port_rets.append(pr)\n",
    "        prev_w = w\n",
    "    if not port_rets:\n",
    "        raise ValueError('No backtest segments computed')\n",
    "    r = pd.concat(port_rets).sort_index()\n",
    "    curve = (1+r).cumprod()\n",
    "    return {'returns': r, 'curve': curve, 'weights': weights_rec}\n",
    "\n",
    "# === Mean–Variance weights via risk aversion γ (SLSQP fallback) ===\n",
    "from scipy.optimize import minimize\n",
    "def mv_weights_gamma(mu: np.ndarray, Sigma: np.ndarray, gamma: float = 5.0, long_only=True):\n",
    "    n = len(mu)\n",
    "    def obj(w): return -(w@mu - gamma*(w@Sigma@w))\n",
    "    cons = ({'type':'eq','fun':lambda w: np.sum(w)-1.0},)\n",
    "    bnds = [(0,1)]*n if long_only else None\n",
    "    res = minimize(obj, x0=np.ones(n)/n, bounds=bnds, constraints=cons, method='SLSQP')\n",
    "    return res.x\n",
    "\n",
    "# === Risk measures (historical VaR / CVaR) ===\n",
    "def var_historical(series: pd.Series, alpha=0.05):\n",
    "    return -np.quantile(series.dropna(), alpha)\n",
    "def cvar_empirical(series: pd.Series, alpha=0.05):\n",
    "    q = np.quantile(series.dropna(), alpha)\n",
    "    tail = series[series <= q]\n",
    "    return -tail.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff35c60",
   "metadata": {},
   "source": [
    "## 1) Data & Returns — Live Download with Robust Fallback\n",
    "**We fetch Adjusted Close prices from Yahoo Finance** and compute **log returns**.\n",
    "\n",
    "### Why log returns?\n",
    "$r_t = \\ln(P_t/P_{t-1})$ are **additive** over time, symmetric for small changes, and often closer to Normal.\n",
    "Annualization uses **252 trading days** for daily data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfd540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tickers = ['AAPL','MSFT','AMZN','GOOG','META','SPY']\n",
    "start_date = '2016-01-01'\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    px = yf.download(tickers, start=start_date, progress=False)['Adj Close']\n",
    "    if isinstance(px.columns, pd.MultiIndex):\n",
    "        px = px.droplevel(0, axis=1)\n",
    "    px = px.dropna()\n",
    "    source = 'Yahoo Finance'\n",
    "except Exception as e:\n",
    "    # Synthetic fallback (so the lab is always runnable)\n",
    "    dates = pd.date_range(start_date, periods=2200, freq='B')\n",
    "    rng = np.random.default_rng(42)\n",
    "    rets_syn = pd.DataFrame(rng.normal(0.0004, 0.015, size=(len(dates), len(tickers))), index=dates, columns=tickers)\n",
    "    px = (1+rets_syn).cumprod()\n",
    "    source = f'Synthetic (fallback due to: {e})'\n",
    "\n",
    "def log_returns(prices: pd.DataFrame):\n",
    "    return np.log(prices/prices.shift(1)).dropna()\n",
    "\n",
    "R = log_returns(px)\n",
    "mu_ann = R.mean()*252\n",
    "cov_ann = R.cov()*252\n",
    "px.tail(), source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3470c38",
   "metadata": {},
   "source": [
    "### Interpreting the data\n",
    "- **Tech-heavy set** (AAPL, AMZN, GOOG, META) will often be **correlated**, so diversification *within* tech is limited.\n",
    "- **SPY** (broad market ETF) provides an anchor for market risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0136692",
   "metadata": {},
   "source": [
    "## 2) Baseline Risk Metrics & Tails\n",
    "**What we measure**\n",
    "- **Volatility (σ):** dispersion of returns\n",
    "- **Sharpe Ratio:** reward per unit of risk, $\\frac{E[r]}{\\sigma}$ (use excess over $r_f$ if available)\n",
    "- **VaR (α):** loss threshold not exceeded α% of the time\n",
    "- **CVaR (α):** average loss *beyond* VaR — *severity* of tail events\n",
    "\n",
    "**Regulatory context:** Basel frameworks often use **99%** VaR/CVaR; discretionary PMs may use **95%**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-Weight baseline (naïve diversification)\n",
    "w_ew = np.ones(len(mu_ann))/len(mu_ann)\n",
    "r_ew = R @ w_ew\n",
    "sharpe_ew = np.sqrt(252)*r_ew.mean()/ (r_ew.std()+1e-12)\n",
    "vaR5 = var_historical(r_ew, 0.05)\n",
    "cvaR5 = cvar_empirical(r_ew, 0.05)\n",
    "pd.Series({'Sharpe_EW':sharpe_ew, 'VaR5%':vaR5, 'CVaR5%':cvaR5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ead9c",
   "metadata": {},
   "source": [
    "###  Understanding Return Distributions\n",
    "- Histogram shows frequency of returns; **fat tails** imply more extreme losses than Normal.\n",
    "- QQ-plot compares empirical quantiles to Normal — **curved tails** signal non-Normality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = R['AAPL'].hist(bins=50)\n",
    "plt.title('AAPL — Daily Returns Histogram')\n",
    "plt.show()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "sm.qqplot(R['AAPL'], line='s')\n",
    "plt.title('AAPL — QQ Plot (Normal vs Empirical)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be376ad6",
   "metadata": {},
   "source": [
    "## 3) Portfolio Optimization — Mean–Variance vs Equal-Weight\n",
    "**Theory recap**\n",
    "\\[ \\max_w \\; \\mu^\\top w - \\gamma \\, w^\\top \\Sigma w \\]\n",
    "- $\\mu$: expected returns; $\\Sigma$: covariance matrix\n",
    "- $\\gamma$: **risk aversion** — higher ⇒ more penalty on volatility\n",
    "\n",
    "**Intuition:** EW assumes all assets are equally attractive; MV uses *information* in $\\mu$ and $\\Sigma$ to tilt weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = mu_ann.index.tolist()\n",
    "gamma = 5.0  # adjust below with a slider\n",
    "w_mv = mv_weights_gamma(mu_ann.values, cov_ann.values, gamma=gamma, long_only=True)\n",
    "pd.Series(w_mv, index=assets, name=f'w_MV_gamma={gamma}').sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2200e",
   "metadata": {},
   "source": [
    "## 4) Backtest — Monthly Rebalance, Transaction Costs\n",
    "**What is backtesting?** Simulating historical performance with realistic assumptions.\n",
    "\n",
    "**Why monthly?** Reasonable balance between responsiveness and transaction costs.\n",
    "**Costs** measured in **bps** (basis points): 5 bps = 0.05% applied at rebalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb216608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wf_ew(window):\n",
    "    return np.ones(len(assets))/len(assets)\n",
    "\n",
    "def wf_mv(window):\n",
    "    r = np.log(window/window.shift(1)).dropna()\n",
    "    mu_w = r.mean()*252\n",
    "    cov_w = r.cov()*252\n",
    "    return mv_weights_gamma(mu_w.values, cov_w.values, gamma=gamma, long_only=True)\n",
    "\n",
    "bt_ew = rebalance_backtest(R[assets], wf_ew, rebalance_freq='M', tx_bps=5)\n",
    "bt_mv = rebalance_backtest(R[assets], wf_mv, rebalance_freq='M', tx_bps=5)\n",
    "bt_ew['curve'].tail(), bt_mv['curve'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e44d9",
   "metadata": {},
   "source": [
    "### 4.1 Cumulative & Drawdown Charts — How to Read Them\n",
    "- **Cumulative growth** shows wealth over time; higher is better, but beware of volatility.\n",
    "- **Drawdown** is peak-to-trough loss; shallower drawdowns = better downside protection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d265149",
   "metadata": {},
   "outputs": [],
   "source": [
    "curves = {'EW': bt_ew['curve'], f'MV(γ={gamma})': bt_mv['curve']}\n",
    "if _HAVE_PLOTLY:\n",
    "    df = pd.concat(curves, axis=1)\n",
    "    df.columns.name = 'Strategy'\n",
    "    melted = df.reset_index().melt(id_vars=['index'], var_name='Strategy', value_name='Growth')\n",
    "    fig = px.line(melted, x='index', y='Growth', color='Strategy', title='Cumulative Performance — Interactive')\n",
    "    fig\n",
    "else:\n",
    "    for k,v in curves.items(): plt.plot(v, label=k)\n",
    "    plt.title('Cumulative Performance')\n",
    "    plt.legend(); plt.show()\n",
    "    export_fig(name='cumulative_performance', fmt='png', out_dir='reports')\n",
    "\n",
    "def dd(s): return s/s.cummax()-1.0\n",
    "if _HAVE_PLOTLY:\n",
    "    dd_df = pd.concat({k: dd(v) for k,v in curves.items()}, axis=1)\n",
    "    dd_df.columns.name = 'Strategy'\n",
    "    ddm = dd_df.reset_index().melt(id_vars=['index'], var_name='Strategy', value_name='Drawdown')\n",
    "    fig2 = px.line(ddm, x='index', y='Drawdown', color='Strategy', title='Drawdown — Interactive')\n",
    "    fig2\n",
    "else:\n",
    "    for k,v in curves.items(): plt.plot(dd(v), label=f'{k} DD')\n",
    "    plt.title('Drawdown Comparison')\n",
    "    plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb23c0",
   "metadata": {},
   "source": [
    "### 4.2 Rolling Sharpe (252-day window)\n",
    "**Why it matters:** Captures *stability* of performance. Persistent values above 1.0 generally indicate robust alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54549f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = rolling_sharpe(bt_mv['returns'], window=252)\n",
    "plt.plot(rs); plt.title('Rolling Sharpe (MV, 252-day)'); plt.show()\n",
    "export_fig(name='rolling_sharpe_mv_252', fmt='png', out_dir='reports')\n",
    "show_summary_stats(bt_mv['returns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e5266",
   "metadata": {},
   "source": [
    "## 5) Interactive Widgets — Build Intuition Live\n",
    "These controls let you *see* how risk settings change outcomes, without re-running the entire notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8027d6f",
   "metadata": {},
   "source": [
    "###  VaR Confidence Level (α)\n",
    "- **What it does:** Changes the probability threshold for extreme losses.\n",
    "- **How to interpret:** Smaller α (e.g., 1–5%) focuses on *rarer, larger* losses.\n",
    "- **Regulatory context:** VaR at 99% is common in banking risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c5111",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _HAVE_WIDGETS:\n",
    "    alpha = widgets.FloatSlider(value=0.05, min=0.01, max=0.20, step=0.01,\n",
    "                                description='VaR α', style={'description_width':'initial'},\n",
    "                                tooltip='Adjusts VaR confidence; smaller α = rarer, larger tail risk')\n",
    "    def plot_var(a):\n",
    "        thr = np.quantile(bt_mv['returns'].dropna(), a)\n",
    "        plt.figure(figsize=(7,3))\n",
    "        bt_mv['returns'].hist(bins=50)\n",
    "        plt.axvline(thr, linestyle='--', label=f'VaR {a*100:.1f}%')\n",
    "        plt.legend(); plt.title('Interactive Historical VaR (MV)'); plt.show()\n",
    "    out = widgets.interactive_output(plot_var, {'a': alpha})\n",
    "    display(alpha); display(out)\n",
    "else:\n",
    "    print('ipywidgets not available — install and reload the notebook to enable sliders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a3f46",
   "metadata": {},
   "source": [
    "###  Risk Aversion (γ) — Impact on Weights\n",
    "- **Low γ (≈1–3):** risk-tolerant → more weight to higher-return, higher-volatility assets\n",
    "- **High γ (≥10):** risk-averse → more diversified, lower-volatility allocations\n",
    "**Tip:** Watch how weights rotate from growth names to broader market exposure as γ increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _HAVE_WIDGETS:\n",
    "    gamma_slider = widgets.FloatSlider(value=5.0, min=0.1, max=25.0, step=0.1,\n",
    "                                       description='γ (risk aversion)', style={'description_width':'initial'},\n",
    "                                       tooltip='Higher γ penalizes variance more strongly')\n",
    "    def solve_and_plot(g):\n",
    "        w = mv_weights_gamma(mu_ann.values, cov_ann.values, gamma=g, long_only=True)\n",
    "        plt.figure(figsize=(9,3))\n",
    "        plt.bar(assets, w)\n",
    "        plt.xticks(rotation=45, ha='right'); plt.title(f'MV Weights (γ={g:.1f})')\n",
    "        plt.tight_layout(); plt.show()\n",
    "    out2 = widgets.interactive_output(solve_and_plot, {'g': gamma_slider})\n",
    "    display(gamma_slider); display(out2)\n",
    "else:\n",
    "    print('ipywidgets not available — install and reload the notebook to enable sliders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c81f7",
   "metadata": {},
   "source": [
    "## 6) Interpreting Results & What to Try Next\n",
    "**What you learned**\n",
    "- Quantified portfolio risk (σ, Sharpe, VaR, CVaR) and *why tails matter*\n",
    "- How risk aversion (γ) translates into portfolio tilts\n",
    "- How to read cumulative vs drawdown plots and rolling Sharpe\n",
    "\n",
    "**Next steps**\n",
    "- Try **CVaR optimization** (tail-risk minimization)\n",
    "- Add **Black–Litterman** views to bake in your beliefs about relative performance\n",
    "- Run **factor regressions** (CAPM/FF3/FF5) to separate α (skill) from β (systematic risk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc1137",
   "metadata": {},
   "source": [
    "###  Challenges (optional)\n",
    "1. Change `rebalance_freq` from `'M'` to `'W'` — does performance improve after costs?\n",
    "2. Increase transaction costs from 5 to 25 bps — which strategy is more resilient to friction?\n",
    "3. Replace Mean–Variance with **Equal Risk Contribution** or **CVaR** and compare.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
